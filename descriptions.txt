## Descriptions of the different trainings ##
<name_of_training> - <description_in_natural_language>

<serrano-15> - Training 256x256 imgs with a subset of the training data to see if it converges better than using the whole train dataset
<serrano-16> - The same as test 15, but now with 2000 epochs instead of 200, to see if it does something else
<serrano-17> - Taking the folder of serrano-16, and leaving only 500 epochs (out of the 2000) to test the model at that point

<serrano-17v2> - Training with 1k epochs, train_subset and bigger learning rate (10^-4) to see if it learns colors
<serrano-18> - The same hyperparameters as 17v2, but now using mini-serrano dataset
<serrano-19> - The same as 18, but now with the dataset with balanced glossinesses.

<serrano-20> - Small training to test if now the balanced-glossiness dataset is properly built
<serrano-21> - Big training (5k ep) with mini-serrano to test if it is able to converge with that subset of the data

<serrano-22> - Test with lr 0.0001
<serrano-24> - Test with lr 0.005
<serrano-23> - Test with lr 0.0005

<serrano-25> - Test with discriminator lr 0.000001
<serrano-26> - Test with discriminator lr 0.000005
<serrano-27> - Test with discriminator lr 0.00001

<serrano-28> - Same hyperparameters as serrano-23, but now without computing test loss
<serrano-29> - Trying to do the same as serrano-28 but with the no_grad() statement

<serrano-30> - Same hyperparameters as serrano-23, but using masked-serrano as dataset
<serrano-31> - Like serrano-30, but using Z=10, to see if we can learn masked-serrano with a 10D latent space
<serrano-32> - Like serrano-31, but with 5000 epochs to see how much can it converge
<serrano-33> - Like serrano-30, but using full-masked-serrano

<serrano-34> - Same hyperparameters, but using masked-serrano2
<serrano-35> - Same hyperparameters, but using pattern-serrano
<serrano-36> - Same hyperparameters, but using masked-serrano3

<serrano-37> - Same as masked-serrano (serrano-30), but with gaussian distribution to compute recon_loss
<serrano-38> - Same as masked-serrano (serrano-30), but with laplace distribution to compute recon_loss

<serrano-39> - Same as masked-serrano (serrano-30), but with an implementation of a "sampled" recon_loss
<serrano-40> - Implementation 3 of "normalized" recon_loss, with a factor of 100
<serrano-41> - Same as serrano-40, but with a factor of 1 (only getting the mean)

<serrano-42> - Training like serrano-30 but only for 100 epochs (to make sure nothing is broken so far)
<serrnao-43> - To debug de feature of applying the mask only for recon_loss
<serrano-44> - Training like serrano-30 but with the 4th implementation for the recon_loss

<serrano-45> - Training like serrano-30 with masked-blobs dataset
<serrano-46> - Training like serrano-30 with masked-spheres dataset
<serrano-47> - Training like serrano-30 with the masked-serrano in grayscale dataset
<serrano-48> - The same configuration as serrano-46, to see if we can get another organization of the latent space

<serrano-49> - Training masked-serrano with the recon_loss implementation 3, and trained with similar hyperparameters as MNIST
<serrano-50> - The same as serrano-49, but with a slightly different factor parameter

<serrano-52> - Training with the masked-buddhas dataset
<serrano-53> - Training with the masked-dragons dataset
<serrano-54> - Training with the gray-blobs dataset
<serrano-55> - Training with the gray-spheres dataset
<serrano-56> - Training with the gray-buddhas dataset
<serrano-57> - Training with the gray-dragons dataset
